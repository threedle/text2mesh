<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Text2Mesh Text-Driven Neural Stylization for Meshes | Oscar Michel1, Roi Bar-On1,2, Richard Liu*1, Sagie Benaim2, Rana Hanocka1</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Text2Mesh Text-Driven Neural Stylization for Meshes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Oscar Michel1, Roi Bar-On1,2, Richard Liu*1, Sagie Benaim2, Rana Hanocka1" />
<meta property="og:description" content="Oscar Michel1, Roi Bar-On1,2, Richard Liu*1, Sagie Benaim2, Rana Hanocka1" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Text2Mesh Text-Driven Neural Stylization for Meshes" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Text2Mesh Text-Driven Neural Stylization for Meshes" />
<script type="application/ld+json">
{"description":"Oscar Michel1, Roi Bar-On1,2, Richard Liu*1, Sagie Benaim2, Rana Hanocka1","url":"http://localhost:4000/","@type":"WebSite","headline":"Text2Mesh Text-Driven Neural Stylization for Meshes","name":"Text2Mesh Text-Driven Neural Stylization for Meshes","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=2bbce57950b79bf92ee648813e784f200e79cb4a">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Text2Mesh: Text-Driven Neural Stylization for Meshes</h1>
      <h2 class="project-tagline">Oscar Michel*<sup>1</sup>, Roi Bar-On*<sup>1,2</sup>, Richard Liu*<sup>1</sup>, Sagie Benaim<sup>2</sup>, Rana Hanocka<sup>1</sup></h2>
      <p class="project-affiliation"><sup>1</sup>University Of Chicago, &nbsp; <sup>2</sup>Tel Aviv University</p>
      <p class="project-thanks">*Equal contribution.</p>

      
    </header>

    <main id="content" class="main-content" role="main">
      <center>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/teaser/ironman_crop.gif" alt="ironman" width="150" /><figcaption style="text-align:center">Iron Man</figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/teaser/candle_crop.gif" alt="a candle made of colorful crochet" width="150" /><figcaption style="text-align:center">Colorful Crochet Candle</figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/teaser/lamp_crop.gif" alt="a lamp made of brick" width="250" /><figcaption style="text-align:center">Brick Lamp</figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/teaser/horse_crop.gif" alt="a horse wearing an astronaut suit" width="200" /><figcaption style="text-align:center">Astronaut Horse</figcaption></figure>
<p><em>
Text2Mesh produces color and geometric details over a variety of source meshes, driven by a target text prompt. Our stylization results coherently blend unique and ostensibly unrelated combinations of text, capturing both global semantics and part-aware attributes.
</em></p>

<a href="https://github.com/threedle/text2mesh" class="btn">Code</a>
<a href="https://www.dropbox.com/s/hbzq7xp95tr1788/text2mesh-preprint.pdf?dl=0" class="btn">Paper</a>
<a href="arxiv.com/supp" class="btn">Supplementary</a>

</center>

<hr />

<h2 id="abstract">Abstract</h2>

<p>In this work, we develop intuitive controls for editing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geometric details which conform to a target text prompt. We consider a disentangled representation of a 3D object using a fixed mesh input (content) coupled with a learned neural network, which we term neural style field network. In order to modify style, we obtain a similarity score between a text prompt (describing style) and a stylized mesh by harnessing the representational power of CLIP. Text2Mesh requires neither a pre-trained  generative model nor a specialized 3D mesh dataset. It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demonstrate the ability of our technique to synthesize a myriad of styles over a wide variety of 3D meshes.</p>

<h2 id="overview">Overview</h2>
<center>
<img src="figures/pipeline.jpg" alt="Pipeline" width="1000" />
<p><em>Text2Mesh modifies an <span style="color: palegreen">input mesh</span> to conform to the <span style="color: palegreen">target text</span> by predicting color and geometric details. The weights of the <span style="color: sandybrown">neural style network</span> are optimized by <span style="color: royalblue">rendering</span> multiple 2D images and applying <span style="color: royalblue">2D augmentations</span>, which are given a similarity score to the target from the CLIP-based <span style="color: salmon">semantic loss</span>.</em></p>
</center>

<h2 id="view-consistency">View Consistency</h2>
<p>We use <a href="https://openai.com/blog/clip/">CLIP’s</a> ability to jointly embed text and images to produce view-consistent and semantically meaningful stylizations over the entire 3D shape.</p>
<center>
<figure style="display:inline-block;margin:0;padding:0">
    <img src="figures/multiple-views/croissant_final_crop.gif" alt="croissant made of colorful crochet" width="250" />
    <figcaption style="text-align:center">croissant made of colorful crochet</figcaption>
</figure>
<figure style="display:inline-block;margin:0;padding:0">
    <img src="figures/multiple-views/armadillo_final_crop.gif" alt="armadillo made of gold" width="250" />
    <figcaption style="text-align:center">armadillo made of gold</figcaption>
</figure>
<figure style="display:inline-block;margin:0;padding:0">
    <img src="figures/multiple-views/donkey_final_crop.gif" alt="donkey wearing jeans" width="250" />
    <figcaption style="text-align:center">donkey wearing jeans</figcaption>
</figure>
</center>

<h2 id="general-stylization">General Stylization</h2>
<p>For the same input mesh, Text2Mesh is capable of generating a variety of different local geometric displacements to synthesize a wide range of styles.</p>
<center>
<div class="double-carousel">
    <div class="item">
        <img src="figures/morphs/vase_init_crop.gif" alt="vase" width="400" />
        <img src="figures/morphs/vase_full_crop.gif" alt="vase" width="400" />
    </div>
    <div class="item">
        <img src="figures/morphs/donut_init_crop.gif" alt="donut" width="400" />
        <img src="figures/morphs/donut_full_crop.gif" alt="donut" width="400" />
    </div>
    <div class="item">
        <img src="figures/morphs/camel_init_crop.gif" alt="camel" width="400" />
        <img src="figures/morphs/camel_full_crop.gif" alt="camel" width="400" />
    </div>
    <div class="item">
        <img src="figures/morphs/chair_init_crop.gif" alt="chair" width="400" />
        <img src="figures/morphs/chair_full_crop.gif" alt="chair" width="400" />
    </div>
    <div class="item">
        <img src="figures/morphs/alien_init_crop.gif" alt="alien" width="400" />
        <img src="figures/morphs/alien_full_crop.gif" alt="alien" width="400" />
    </div>
 </div>
</center>

<h2 id="ablations">Ablations</h2>
<p>We show the distinct effect of each of our design choices on the quality of the final stylization through a series of ablations.</p>
<center>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/ablation/candle_base_crop.gif" width="90" /><figcaption style="text-align:center">source</figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/ablation/candle_full_crop.gif" width="100" /><figcaption style="text-align:center"><i>full</i></figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/ablation/candle_ablnetwork_crop.gif" width="100" /><figcaption style="text-align:center"><i>-net</i></figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/ablation/candle_ablaug_crop.gif" width="100" /><figcaption style="text-align:center"><i>-aug</i></figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/ablation/candle_ablffn_crop.gif" width="100" /><figcaption style="text-align:center"><i>-FFN</i></figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/ablation/candle_nocrop_crop.gif" width="100" /><figcaption style="text-align:center"><i>-crop</i></figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/ablation/candle_nogeo_crop.gif" width="100" /><figcaption style="text-align:center"><i>-displ</i></figcaption></figure>
<figure style="display:inline-block;margin:0;padding:0"><img src="figures/ablation/candle_ablplane_crop.gif" width="100" /><figcaption style="text-align:center"><i>-3D</i></figcaption></figure>
<p><em>Ablation on the priors used in our method (<i>full</i>) for a candle mesh and target ‘Candle made of bark’: w/o our style field network (<i>−net</i>), w/o 2D augmentations (<i>−aug</i>), w/o positional encoding (<i>−FFN</i>), w/o crop augmentations for ψlocal (<i>−crop</i>), w/o the geometry-only component of Lsim (<i>−displ</i>), and learning over a 2D plane in 3D space (<i>−3D</i>).</em></p>
</center>

<h2 id="beyond-text-driven-manipulation">Beyond Text-Driven Manipulation</h2>
<p>We further leverage the joint vision-language embedding space to demonstrate the multi-modal stylization capabilities of our method.</p>

<h3 id="image-and-mesh-targets">Image and Mesh Targets</h3>
<center>
<figure style="display:inline-block;margin:0;padding:0">
    <img src="figures/target-image/bucket_cobble.gif" alt="bucket" width="150" />
    <figcaption style="text-align:center">image target</figcaption>
</figure>
<figure style="display:inline-block;margin:0;padding:0">
    <img src="figures/target-image/pig_fish.gif" alt="pig" width="220" />
    <figcaption style="text-align:center">image target</figcaption>
</figure>
<figure style="display:inline-block;margin:0;padding:0">
    <img src="figures/target-image/iron_crochet.gif" alt="iron" width="200" />
    <figcaption style="text-align:center">image target</figcaption>
</figure>
<figure style="display:inline-block;margin:0;padding:0">
    <img src="figures/target-mesh/armadillo_final.gif" alt="armadillo" width="220" />
    <figcaption>mesh target</figcaption>
</figure>
</center>

<h2 id="citation">Citation</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{text2mesh,
    author = {Michel, Oscar
              and Bar-On, Roi
              and Liu, Richard
              and Benaim, Sagie
              and Hanocka, Rana
              },
    title = {Text2Mesh: Text-Driven Neural Stylization for Meshes},
    journal = {TODO: ARXIV},
    year  = {2021}
}
</code></pre></div></div>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/threedle/text2mesh">text2mesh</a> is maintained by <a href="https://github.com/threedle">threedle</a>.</span>
        
      </footer>
    </main>
  </body>
</html>